{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA GENERATION\n",
    "p = 500\n",
    "N = 10**5\n",
    "cov = 2 * np.identity(p)\n",
    "mean = np.ones(p)\n",
    "X = random.multivariate_normal(mean, cov, size=N)\n",
    "nnz = int(p/25)\n",
    "nz_nums = random.multivariate_normal(np.ones(nnz), np.identity(nnz), size=1)\n",
    "indexes = random.choice(range(p), size=nnz, replace=False, p=None)\n",
    "ground_beta = np.zeros((p,1))\n",
    "ground_beta[indexes,0] = nz_nums\n",
    "from numpy import matmul\n",
    "y = matmul(X,ground_beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Implement subgradient descent algorithm with your choice of stepsize. Try a fixed stepsize and a decreasing stepsize. Plot the objective function with growing number of iterations for each algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "from pdb import set_trace\n",
    "\n",
    "lam = 0.1 # this is lambda\n",
    "thresh = 0.1\n",
    "mean = np.zeros(p)\n",
    "cov = np.identity(p)\n",
    "beta_0 = random.multivariate_normal(mean, cov, size=1).reshape((p,1))\n",
    "\n",
    "def loss(X, y, beta, lam):\n",
    "#     set_trace()\n",
    "    return   (1/N) * 0.5 * norm((matmul(X,beta)-y), ord=2)**2  + lam * norm(beta, ord=1) \n",
    "\n",
    "def subgrad(X, y, beta, lam):\n",
    "    # gradient of the loss function\n",
    "    return   (1/N) * matmul(np.transpose(X),(matmul(X,beta)-y)) + lam * np.sign(beta) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function loss at 0x7ff0a2ad7b70>\n",
      "loss = 615.8631102163878\n",
      "loss = 591.6247485947491\n",
      "loss = 568.3540114001677\n",
      "loss = 546.0329313392489\n",
      "loss = 524.5940040589243\n",
      "loss = 504.0000603057408\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-3fdf925cb5d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mbeta_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mbeta\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msubgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mL_min\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mL_min\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-7916457b5d28>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(X, y, beta, lam)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#     set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m   \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# FIXED ALPHA\n",
    "alpha = 0.001\n",
    "beta = np.empty_like(beta_0)\n",
    "beta[:] = beta_0\n",
    "L = loss(X, y, beta, lam)\n",
    "L_min = [L,beta]\n",
    "L1_store = []\n",
    "c1_store = []\n",
    "print (loss)\n",
    "counter = 0\n",
    "while L > thresh:\n",
    "    beta_old = beta\n",
    "    beta -= alpha * subgrad(X, y, beta, lam)\n",
    "    L = loss(X, y, beta, lam)\n",
    "    if L < L_min[0]:\n",
    "        L_min[0] = L\n",
    "        L_min[1] = beta\n",
    "    else:\n",
    "        beta = beta_old\n",
    "    counter += 1\n",
    "    if counter % 10 == 0:\n",
    "        L1_store.append(L)\n",
    "        c1_store.append(counter)\n",
    "        print (\"loss = {}\".format(L))\n",
    "print(\"iterations for fixed alpha done\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 570.0389047756707\n",
      "loss = 553.8705544369836\n",
      "loss = 544.7714834450585\n",
      "loss = 538.4453943878948\n",
      "loss = 533.6052327430299\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-1bdd46f03f06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mbeta_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mbeta\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msubgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha_0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0malpha_0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-7916457b5d28>\u001b[0m in \u001b[0;36msubgrad\u001b[0;34m(X, y, beta, lam)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# gradient of the loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mreturn\u001b[0m   \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# DECREASING ALPHA\n",
    "alpha_0 = 0.01\n",
    "beta = np.empty_like(beta_0)\n",
    "beta[:] = beta_0\n",
    "L = loss(X, y, beta, lam)\n",
    "L_min = [L,beta]\n",
    "L2_store = []\n",
    "c2_store = []\n",
    "counter = 0\n",
    "while L > thresh:\n",
    "    counter += 1\n",
    "    beta_old = beta\n",
    "    beta -= alpha * subgrad(X, y, beta, lam)\n",
    "    alpha = alpha_0 / (alpha_0 + counter)\n",
    "    L = loss(X, y, beta, lam)\n",
    "    if L < L_min[0]:\n",
    "        L_min[0] = L\n",
    "        L_min[1] = beta\n",
    "    else:\n",
    "        beta = beta_old\n",
    "    if counter % 10 == 0:\n",
    "        L2_store.append(L)\n",
    "        c2_store.append(counter)\n",
    "        print (\"loss = {}\".format(L))\n",
    "print(\"iterations for varying alpha done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Implement proximal gradient descent with your choice of stepsize. Plot the objective function with growing number of iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_thresh(lam, alpha, beta):\n",
    "    m, _ = np.shape(beta)\n",
    "    s_t = np.zeros((m,1))\n",
    "    for i in range(m):\n",
    "        if beta[i] > lam * alpha:\n",
    "            s_t[i,0] = beta[i] - lam * alpha\n",
    "        elif beta[i] < - lam * alpha:\n",
    "            s_t[i,0] = beta[i] + lam * alpha\n",
    "        else:\n",
    "            s_t[i,0] = 0\n",
    "    return s_t\n",
    "\n",
    "def g(beta):\n",
    "    # corresponds to the smooth term in the loss\n",
    "    return   (1/N) * 0.5 * norm((matmul(X,beta)-y), ord=2)**2 \n",
    "\n",
    "def grad_g(beta):\n",
    "    # corresponds to the gradient of the smooth term in the loss\n",
    "    return   (1/N) * matmul(np.transpose(X),(matmul(X,beta)-y)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 492.7994930742372\n",
      "loss = 482.30580410399915\n",
      "loss = 472.34709625428366\n",
      "loss = 462.5927960148957\n",
      "loss = 453.04110126969573\n",
      "loss = 443.6840008469633\n",
      "loss = 434.5175803542272\n",
      "loss = 425.5380638977054\n",
      "loss = 416.74154462981966\n",
      "loss = 408.1243128533027\n",
      "loss = 399.6827355843921\n",
      "loss = 391.4132534577697\n",
      "loss = 383.31238467340853\n",
      "loss = 375.37676586536656\n",
      "loss = 367.60464495835635\n",
      "loss = 359.99697706695633\n",
      "loss = 352.5446345783274\n",
      "loss = 345.2442972850416\n",
      "loss = 338.09284543108856\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-49b1d13059f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad_g\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoft_thresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-53a7ef0d191b>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(X, y, beta, lam)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#     set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m   \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "alpha = 0.001\n",
    "beta = np.empty_like(beta_0)\n",
    "beta[:] = beta_0\n",
    "L = loss(X, y, beta, lam)\n",
    "L3_store = []\n",
    "c3_store = []\n",
    "counter = 0\n",
    "while L > thresh:\n",
    "    counter += 1\n",
    "    beta = beta - alpha * grad_g(beta)\n",
    "    beta = soft_thresh(lam, alpha, beta)       \n",
    "    L = loss(X, y, beta, lam)\n",
    "    counter += 1\n",
    "    if counter % 10 == 0:\n",
    "        L3_store.append(L)\n",
    "        c3_store.append(counter)\n",
    "        print (\"loss = {}\".format(L))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Implement proximal gradient descent with backtracking line search. You can find more about backtracking line search in https://www.robots.ox.ac. uk/~vgg/rg/slides/fgrad.pdf. Plot the objective function with growing number of iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 482.31865637570826\n",
      "alpha = 0.001996678111016037\n",
      "loss = 462.6192350340075\n",
      "alpha = 0.001996678111016037\n",
      "loss = 443.7215069050127\n",
      "alpha = 0.001996678111016037\n",
      "loss = 425.5857623191486\n",
      "alpha = 0.001996678111016037\n",
      "loss = 408.18127317255926\n",
      "alpha = 0.001996678111016037\n",
      "loss = 391.478656325768\n",
      "alpha = 0.001996678111016037\n",
      "loss = 375.44986349472424\n",
      "alpha = 0.001996678111016037\n",
      "loss = 360.07702168926363\n",
      "alpha = 0.001996678111016037\n",
      "loss = 345.3306427136509\n",
      "alpha = 0.001996678111016037\n",
      "loss = 331.17924107598367\n",
      "alpha = 0.001996678111016037\n",
      "loss = 317.5989734925165\n",
      "alpha = 0.001996678111016037\n",
      "loss = 304.56691833174557\n",
      "alpha = 0.001996678111016037\n",
      "loss = 292.0622324926676\n",
      "alpha = 0.001996678111016037\n",
      "loss = 280.0669819408345\n",
      "alpha = 0.001996678111016037\n",
      "loss = 268.5564269592902\n",
      "alpha = 0.001996678111016037\n",
      "loss = 257.51115092538123\n",
      "alpha = 0.001996678111016037\n",
      "loss = 246.91237200316155\n",
      "alpha = 0.001996678111016037\n",
      "loss = 236.74459190482872\n",
      "alpha = 0.001996678111016037\n",
      "loss = 226.99154355945694\n",
      "alpha = 0.001996678111016037\n",
      "loss = 217.63304434805252\n",
      "alpha = 0.001996678111016037\n",
      "loss = 208.65325548841477\n",
      "alpha = 0.001996678111016037\n",
      "loss = 200.0371810519304\n",
      "alpha = 0.001996678111016037\n",
      "loss = 191.77010628035697\n",
      "alpha = 0.001996678111016037\n",
      "loss = 183.83820938967992\n",
      "alpha = 0.001996678111016037\n",
      "loss = 176.22785683515156\n",
      "alpha = 0.001996678111016037\n",
      "loss = 168.92844270758005\n",
      "alpha = 0.001996678111016037\n",
      "loss = 161.9285173231675\n",
      "alpha = 0.001996678111016037\n",
      "loss = 155.21251968227463\n",
      "alpha = 0.001996678111016037\n",
      "loss = 148.7690795551942\n",
      "alpha = 0.001996678111016037\n",
      "loss = 142.58726123069366\n",
      "alpha = 0.001996678111016037\n",
      "loss = 136.65663334023475\n",
      "alpha = 0.001996678111016037\n",
      "loss = 130.9671501977234\n",
      "alpha = 0.001996678111016037\n",
      "loss = 125.50902754039286\n",
      "alpha = 0.001996678111016037\n",
      "loss = 120.27304607694983\n",
      "alpha = 0.001996678111016037\n",
      "loss = 115.25016932233848\n",
      "alpha = 0.001996678111016037\n",
      "loss = 110.43183581404465\n",
      "alpha = 0.001996678111016037\n",
      "loss = 105.8098951685277\n",
      "alpha = 0.001996678111016037\n",
      "loss = 101.37639450435985\n",
      "alpha = 0.001996678111016037\n",
      "loss = 97.12379424068254\n",
      "alpha = 0.001996678111016037\n",
      "loss = 93.04474392824997\n",
      "alpha = 0.001996678111016037\n",
      "loss = 89.13227034389745\n",
      "alpha = 0.001996678111016037\n",
      "loss = 85.37978201647495\n",
      "alpha = 0.001996678111016037\n",
      "loss = 81.78095216526782\n",
      "alpha = 0.001996678111016037\n",
      "loss = 78.32944397193226\n",
      "alpha = 0.001996678111016037\n",
      "loss = 75.01923850193876\n",
      "alpha = 0.001996678111016037\n",
      "loss = 71.84470920024755\n",
      "alpha = 0.001996678111016037\n",
      "loss = 68.80039530789874\n",
      "alpha = 0.001996678111016037\n",
      "loss = 65.88105670303254\n",
      "alpha = 0.001996678111016037\n",
      "loss = 63.08173570516099\n",
      "alpha = 0.001996678111016037\n",
      "loss = 60.39745881401994\n",
      "alpha = 0.001996678111016037\n",
      "loss = 57.82359486655643\n",
      "alpha = 0.001996678111016037\n",
      "loss = 55.35569820303888\n",
      "alpha = 0.001996678111016037\n",
      "loss = 52.98948251203306\n",
      "alpha = 0.001996678111016037\n",
      "loss = 50.720859981349705\n",
      "alpha = 0.001996678111016037\n",
      "loss = 48.54589434359464\n",
      "alpha = 0.001996678111016037\n",
      "loss = 46.460801392398714\n",
      "alpha = 0.001996678111016037\n",
      "loss = 44.46196085635723\n",
      "alpha = 0.001996678111016037\n",
      "loss = 42.54606864071613\n",
      "alpha = 0.001996678111016037\n",
      "loss = 40.709772445313234\n",
      "alpha = 0.001996678111016037\n",
      "loss = 38.94968415707916\n",
      "alpha = 0.001996678111016037\n",
      "loss = 37.26270984236149\n",
      "alpha = 0.001996678111016037\n",
      "loss = 35.645886789607445\n",
      "alpha = 0.001996678111016037\n",
      "loss = 34.0963926337178\n",
      "alpha = 0.001996678111016037\n",
      "loss = 32.611510331382846\n",
      "alpha = 0.001996678111016037\n",
      "loss = 31.18862500856543\n",
      "alpha = 0.001996678111016037\n",
      "loss = 29.825214286620877\n",
      "alpha = 0.001996678111016037\n",
      "loss = 28.51886794541214\n",
      "alpha = 0.001996678111016037\n",
      "loss = 27.26726830801741\n",
      "alpha = 0.001996678111016037\n",
      "loss = 26.068144573395802\n",
      "alpha = 0.001996678111016037\n",
      "loss = 24.919499361275516\n",
      "alpha = 0.001996678111016037\n",
      "loss = 23.81925083736082\n",
      "alpha = 0.001996678111016037\n",
      "loss = 22.765363989757297\n",
      "alpha = 0.001996678111016037\n",
      "loss = 21.755921182608475\n",
      "alpha = 0.001996678111016037\n",
      "loss = 20.789096182857485\n",
      "alpha = 0.001996678111016037\n",
      "loss = 19.863151195917187\n",
      "alpha = 0.001996678111016037\n",
      "loss = 18.97645990878539\n",
      "alpha = 0.001996678111016037\n",
      "loss = 18.12742333319712\n",
      "alpha = 0.001996678111016037\n",
      "loss = 17.31444043337339\n",
      "alpha = 0.001996678111016037\n",
      "loss = 16.5360305775787\n",
      "alpha = 0.001996678111016037\n",
      "loss = 15.790824155318603\n",
      "alpha = 0.001996678111016037\n",
      "loss = 15.077453638414863\n",
      "alpha = 0.001996678111016037\n",
      "loss = 14.394631494281645\n",
      "alpha = 0.001996678111016037\n",
      "loss = 13.74113181903089\n",
      "alpha = 0.001996678111016037\n",
      "loss = 13.115720259788647\n",
      "alpha = 0.001996678111016037\n",
      "loss = 12.517211241483757\n",
      "alpha = 0.001996678111016037\n",
      "loss = 11.944529636738713\n",
      "alpha = 0.001996678111016037\n",
      "loss = 11.39659161253682\n",
      "alpha = 0.001996678111016037\n",
      "loss = 10.872389494215088\n",
      "alpha = 0.001996678111016037\n",
      "loss = 10.370959833332138\n",
      "alpha = 0.001996678111016037\n",
      "loss = 9.891336251445637\n",
      "alpha = 0.001996678111016037\n",
      "loss = 9.43262557394324\n",
      "alpha = 0.001996678111016037\n",
      "loss = 8.993964811101051\n",
      "alpha = 0.001996678111016037\n",
      "loss = 8.574526043835295\n",
      "alpha = 0.001996678111016037\n",
      "loss = 8.17350869627163\n",
      "alpha = 0.001996678111016037\n",
      "loss = 7.7901883504937715\n",
      "alpha = 0.001996678111016037\n",
      "loss = 7.423803140694996\n",
      "alpha = 0.001996678111016037\n",
      "loss = 7.073673838627339\n",
      "alpha = 0.001996678111016037\n",
      "loss = 6.739114442584978\n",
      "alpha = 0.001996678111016037\n",
      "loss = 6.419423773915183\n",
      "alpha = 0.001996678111016037\n",
      "loss = 6.1139616773629\n",
      "alpha = 0.001996678111016037\n",
      "loss = 5.822157505763544\n",
      "alpha = 0.001996678111016037\n",
      "loss = 5.543417177885371\n",
      "alpha = 0.001996678111016037\n",
      "loss = 5.27724357951053\n",
      "alpha = 0.001996678111016037\n",
      "loss = 5.023139025088071\n",
      "alpha = 0.001996678111016037\n",
      "loss = 4.780542161390524\n",
      "alpha = 0.001996678111016037\n",
      "loss = 4.548996198554294\n",
      "alpha = 0.001996678111016037\n",
      "loss = 4.327994607736591\n",
      "alpha = 0.001996678111016037\n",
      "loss = 4.117069058299728\n",
      "alpha = 0.001996678111016037\n",
      "loss = 3.91579675618561\n",
      "alpha = 0.001996678111016037\n",
      "loss = 3.72377454197727\n",
      "alpha = 0.001996678111016037\n",
      "loss = 3.540637348217384\n",
      "alpha = 0.001996678111016037\n",
      "loss = 3.365988397183502\n",
      "alpha = 0.001996678111016037\n",
      "loss = 3.199461715072669\n",
      "alpha = 0.001996678111016037\n",
      "loss = 3.040677127876632\n",
      "alpha = 0.001996678111016037\n",
      "loss = 2.8893177420927545\n",
      "alpha = 0.001996678111016037\n",
      "loss = 2.7450743641211734\n",
      "alpha = 0.001996678111016037\n",
      "loss = 2.6076426793191216\n",
      "alpha = 0.001996678111016037\n",
      "loss = 2.4767278511657373\n",
      "alpha = 0.001996678111016037\n",
      "loss = 2.3520435519517324\n",
      "alpha = 0.001996678111016037\n",
      "loss = 2.233308621322006\n",
      "alpha = 0.001996678111016037\n",
      "loss = 2.1202902178180536\n",
      "alpha = 0.001996678111016037\n",
      "loss = 2.0126971986034756\n",
      "alpha = 0.001996678111016037\n",
      "loss = 1.9103282016959968\n",
      "alpha = 0.001996678111016037\n",
      "loss = 1.812920087003402\n",
      "alpha = 0.001996678111016037\n",
      "loss = 1.7202754084734104\n",
      "alpha = 0.001996678111016037\n",
      "loss = 1.6321586375014137\n",
      "alpha = 0.001996678111016037\n",
      "loss = 1.5484026267795152\n",
      "alpha = 0.001996678111016037\n",
      "loss = 1.4688276899895918\n",
      "alpha = 0.001996678111016037\n",
      "loss = 1.3932109769209373\n",
      "alpha = 0.001996678111016037\n",
      "loss = 1.3213734261376624\n",
      "alpha = 0.001996678111016037\n",
      "loss = 1.2531439634355792\n",
      "alpha = 0.001996678111016037\n",
      "loss = 1.1883823061493606\n",
      "alpha = 0.001996678111016037\n",
      "loss = 1.1268778798495804\n",
      "alpha = 0.001996678111016037\n",
      "loss = 1.0685127884014292\n",
      "alpha = 0.001996678111016037\n",
      "loss = 1.0131266952928673\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.9605844059928706\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.9107630414519835\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.8635670175272631\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.818845197834506\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.7765210806567512\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.736475063493686\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.69862758341488\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.6628287279526285\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.6289908991339472\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.5969259443308434\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.5665984279860029\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.5379062907485757\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.5107904301110864\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.4851739303689023\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.46101743224027164\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.43820934721781846\n",
      "alpha = 0.001996678111016037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.4166605927410841\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.3963332598584042\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.37714232732918235\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.3590208610782121\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.3419047705286867\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.32577128211114925\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.3106079580299944\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.29639604727331376\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.28296763619298065\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.2702800386513738\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.25828097176519815\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.24708355381801383\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.23646977328595922\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.2265019096917789\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.21700750619255063\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.2080471461509635\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.1995772023603555\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.19153158579748708\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.18399674512085176\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.17686931853961793\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.17000975000306293\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.16358877151690493\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.15738425931803313\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.15160986008074545\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.14611658060102004\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.14083027622780692\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.13589812230493664\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.13142105454982358\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.12702271265835433\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.12264835713438972\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.11844217097584164\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.11431225531339348\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.11031837415230027\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.10647284972399898\n",
      "alpha = 0.001996678111016037\n",
      "loss = 0.10277629121305489\n",
      "alpha = 0.001996678111016037\n"
     ]
    }
   ],
   "source": [
    "alpha = 1\n",
    "multiplier = 0.9\n",
    "beta = np.empty_like(beta_0)\n",
    "beta[:] = beta_0\n",
    "L = loss(X, y, beta, lam)\n",
    "L4_store = []\n",
    "c4_store = []\n",
    "counter = 0\n",
    "\n",
    "while L > thresh:\n",
    "    counter += 1\n",
    "    # while loop is backtracking line search to find optimal alpha\n",
    "    while True:\n",
    "        beta_plus = beta - alpha * grad_g(beta)\n",
    "        value = g(beta_plus) <= g(beta) + matmul(np.transpose(grad_g(beta)), (beta_plus - beta)) + (1/(2*alpha)) * norm(beta_plus-beta)**2\n",
    "        if not value:\n",
    "            alpha *= multiplier\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    beta = beta - alpha * grad_g(beta)\n",
    "    beta = soft_thresh(lam, alpha, beta)       \n",
    "    L = loss(X, y, beta, lam)\n",
    "    counter += 1\n",
    "    if counter % 10 == 0:\n",
    "        L4_store.append(L)\n",
    "        c4_store.append(counter)\n",
    "        print (\"loss = {}\".format(L))\n",
    "        print (\"alpha = {}\".format(alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Now compare these methods with any publicly available software for lasso, e.g. glmfit or lasso or scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "clf = Lasso(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, \n",
    "                           copy_X=True, max_iter=1000, tol=0.001, warm_start=False, positive=False, \n",
    "                           random_state=None, selection=’cyclic’)\n",
    "clf.fit(X,y)\n",
    "y_pred = clf.predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
