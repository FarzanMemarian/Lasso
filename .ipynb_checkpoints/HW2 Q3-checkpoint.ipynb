{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2 Part 3: Lasso\n",
    "\n",
    "Authors:\n",
    "- Farzan Memarian fm6996\n",
    "- Timothy Mahler tam2643\n",
    "- Meghana Venkata Palukuri mvp549\n",
    "\n",
    "Below are the answers for part a-d.\n",
    "Below D is a summary of training time, Lasso loss, MSE, MAE and RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libs\n",
    "import numpy as np\n",
    "import random\n",
    "import time, sys\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "from numpy import linalg as LA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Base functions\n",
    "current_milli_time = lambda: int(round(time.time() * 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape =  (50000, 500)\n",
      "X_test shape =  (50000, 500)\n",
      "y_train shape =  (50000, 1)\n",
      "y_test shape =  (50000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Generate random array\n",
    "def rand_beta_arr(dim, num_nz):\n",
    "    B = np.zeros(dim)\n",
    "    for i in range(num_nz):\n",
    "        B[i] = np.random.uniform(low=-10, high=10, size=(1,)) # Genrate ranomd weights between -10, 10\n",
    "    np.random.shuffle(B)\n",
    "    B = np.reshape(B, (-1, 1))\n",
    "    return B\n",
    "\n",
    "\n",
    "# Generate data\n",
    "N = 10**5 # num data points\n",
    "p = 500 # num variables\n",
    "rand_state = 12345\n",
    "np.random.seed(rand_state) # Set random seet so all numbers stay the same\n",
    "B_true = rand_beta_arr(500, int(p/25)) # true betas\n",
    "\n",
    "# Generate random covariance matrix\n",
    "A = np.random.rand(p, p)\n",
    "cov_matrix = np.dot(A, A.transpose())\n",
    "\n",
    "# Gen X, Y\n",
    "X_orig = np.random.multivariate_normal(mean=np.zeros(p), cov=np.identity(p), size = N)\n",
    "y_orig = np.matmul(X_orig, B_true)\n",
    "\n",
    "# Cross validation\n",
    "lambdas = [1, 0.5, 0.1, 0.01] # Lamdas to test for K-Fold\n",
    "CV_rounds = 5 # number of cross validation rounds\n",
    "kf = KFold(n_splits=CV_rounds, random_state=rand_state)\n",
    "from math import sqrt\n",
    "\n",
    "# Test / Train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_orig, y_orig, test_size = 0.5)\n",
    "\n",
    "\n",
    "# Print Values\n",
    "#print (B_true)\n",
    "# print (X)\n",
    "# print (Y)\n",
    "#print (X_train)\n",
    "#print (y_train)current_milli_time\n",
    "\n",
    "print (\"X_train shape = \", X_train.shape)\n",
    "print (\"X_test shape = \", X_test.shape)\n",
    "print (\"y_train shape = \", y_train.shape)\n",
    "print (\"y_test shape = \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Subgradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold for Best lamda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the best lambda using K-Fold CV\n",
      "testing lambda =  1\n",
      "Subgradient descent for lam =  1\n",
      "Stop with Loss =  113.0367630600889\n",
      "------------------------\n",
      "Subgradient descent for lam =  1\n",
      "Stop with Loss =  117.72272455367366\n",
      "------------------------\n",
      "Subgradient descent for lam =  1\n",
      "Stop with Loss =  112.86861753494348\n",
      "------------------------\n",
      "Subgradient descent for lam =  1\n",
      "Stop with Loss =  112.87658611796641\n",
      "------------------------\n",
      "Subgradient descent for lam =  1\n",
      "Stop with Loss =  114.31059230297646\n",
      "------------------------\n",
      "test_fold done\n",
      "[112.84244870613537, 117.1267485916269, 113.30608282900764, 113.19914597333741, 114.53093483213851]\n",
      "------------------------\n",
      "testing lambda =  0.5\n",
      "Subgradient descent for lam =  0.5\n",
      "Stop with Loss =  59.36497534020995\n",
      "------------------------\n",
      "Subgradient descent for lam =  0.5\n",
      "Stop with Loss =  57.4370959186823\n",
      "------------------------\n",
      "Subgradient descent for lam =  0.5\n",
      "Stop with Loss =  57.0926215917483\n",
      "------------------------\n",
      "Subgradient descent for lam =  0.5\n",
      "Stop with Loss =  57.88251091517184\n",
      "------------------------\n",
      "Subgradient descent for lam =  0.5\n",
      "Stop with Loss =  56.65507967034024\n",
      "------------------------\n",
      "test_fold done\n",
      "[59.30452679742212, 57.274414919443736, 57.20985367013294, 57.99590687102873, 56.71434627773097]\n",
      "------------------------\n",
      "testing lambda =  0.1\n",
      "Subgradient descent for lam =  0.1\n",
      "Stop with Loss =  11.737079509440866\n",
      "------------------------\n",
      "Subgradient descent for lam =  0.1\n",
      "Stop with Loss =  11.749264415208605\n",
      "------------------------\n",
      "Subgradient descent for lam =  0.1\n",
      "Stop with Loss =  11.782981247162109\n",
      "------------------------\n",
      "Subgradient descent for lam =  0.1\n",
      "Stop with Loss =  11.75872241303645\n",
      "------------------------\n",
      "Subgradient descent for lam =  0.1\n",
      "Stop with Loss =  11.74133425550272\n",
      "------------------------\n",
      "test_fold done\n",
      "[11.738580665391499, 11.741655749096985, 11.795748306351442, 11.773256890538176, 11.74988011519958]\n",
      "------------------------\n",
      "testing lambda =  0.01\n",
      "Subgradient descent for lam =  0.01\n",
      "Stop with Loss =  1.649483713121611\n",
      "------------------------\n",
      "Subgradient descent for lam =  0.01\n",
      "Stop with Loss =  1.6516046465576928\n",
      "------------------------\n",
      "Subgradient descent for lam =  0.01\n",
      "Stop with Loss =  1.659027979463258\n",
      "------------------------\n",
      "Subgradient descent for lam =  0.01\n",
      "Stop with Loss =  1.659367391880136\n",
      "------------------------\n",
      "Subgradient descent for lam =  0.01\n",
      "Stop with Loss =  1.6567989287206863\n",
      "------------------------\n",
      "test_fold done\n",
      "[1.6784744089600916, 1.6838798041210787, 1.6954019810068632, 1.7009963306089309, 1.694870180136499]\n",
      "------------------------\n",
      "Losses\n",
      "[114.20107218644917, 57.6998097071517, 11.759824345315538, 1.6907245409666927]\n",
      "Best lambda for subgradient descent =  0.01\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import norm\n",
    "from pdb import set_trace\n",
    "\n",
    "#----------------- Variables -----------------\n",
    "# these parameters are shared between all parts\n",
    "alpha = 0.01\n",
    "beta_0 = np.random.multivariate_normal(np.ones(p), 2 * np.identity(p), size=1).reshape((p,1)) # initial beta\n",
    "\n",
    "MAX_ITERATIONS = 10000\n",
    "TOLERANCE = 0.01\n",
    "\n",
    "\n",
    "#----------------- Functions -----------------\n",
    "# normalized loss function\n",
    "def loss(X, y, beta, lam):\n",
    "    N,_ = np.shape(X)\n",
    "    return   (1/N) * 0.5 * norm((np.matmul(X,beta)-y), ord=2)**2  + lam * norm(beta, ord=1) \n",
    "\n",
    "def subgrad(X, y, beta, lam):\n",
    "    # gradient of the loss function\n",
    "    N,_ = np.shape(X)\n",
    "    return   (1/N) * np.matmul(np.transpose(X),(np.matmul(X, beta)-y)) + lam * np.sign(beta) \n",
    "\n",
    "# Return Beta given X, Y, alpha\n",
    "def subgradient_descent(X, y, lam):\n",
    "    print (\"Subgradient descent for lam = \",lam)\n",
    "    start = current_milli_time()\n",
    "    \n",
    "    beta = np.empty_like(-beta_0)\n",
    "    beta[:] = beta_0\n",
    "\n",
    "    L = loss(X, y, beta, lam)\n",
    "    old_loss = sys.maxsize\n",
    "    L_min = [L, beta]\n",
    "    counter = 0\n",
    "    \n",
    "    while counter < MAX_ITERATIONS and L < old_loss - TOLERANCE:\n",
    "        beta_old = np.empty_like(beta)\n",
    "        beta_old[:] = beta\n",
    "        beta = beta - (alpha * subgrad(X, y, beta, lam))\n",
    "        delta_beta = norm(beta-beta_old) / p\n",
    "        old_loss = L\n",
    "        L = loss(X, y, beta, lam)\n",
    "        if L < L_min[0]:\n",
    "            L_min[0] = L\n",
    "            L_min[1] = beta\n",
    "        else:\n",
    "            beta = beta_old\n",
    "        counter += 1\n",
    "#         if counter % 100 == 0:\n",
    "#             print (\"loss = {}\".format(L))\n",
    "    \n",
    "    print (\"Stop with Loss = \", L)\n",
    "    print (\"------------------------\")\n",
    "    return (beta, current_milli_time() - start)\n",
    "\n",
    "\n",
    "# Find the best lam using K-Fold\n",
    "def find_best_lambda():\n",
    "    print (\"Finding the best lambda using K-Fold CV\")\n",
    "    losses = []\n",
    "    counter = 0\n",
    "    \n",
    "    for lam in lambdas:\n",
    "        test_fold = []\n",
    "        counter += 1\n",
    "        print (\"testing lambda = \", lam)\n",
    "\n",
    "        # K-fold\n",
    "        for train_idx, test_idx in kf.split(X_train):\n",
    "            xtrain, xtest = X_train[train_idx], X_train[test_idx]\n",
    "            ytrain, ytest = y_train[train_idx], y_train[test_idx]\n",
    "\n",
    "            # Run algo\n",
    "            (B, time) = subgradient_descent(xtrain, ytrain, lam)\n",
    "\n",
    "            l = loss(xtest, ytest, B, lam)\n",
    "            test_fold.append(l)\n",
    "\n",
    "        print (\"test_fold done\")\n",
    "        print (test_fold)\n",
    "        print (\"------------------------\")\n",
    "        losses.append(np.mean(test_fold))\n",
    "        \n",
    "    print(\"Losses\")\n",
    "    print (losses)\n",
    "    return lambdas[np.argmin(losses)]\n",
    "    \n",
    "# Best lambda\n",
    "best_lam = find_best_lambda()\n",
    "print(\"Best lambda for subgradient descent = \", best_lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating model\n",
      "Subgradient descent for lam =  0.01\n",
      "Stop with Loss =  1.6521645607909874\n",
      "------------------------\n",
      "\n",
      "------------Results------------\n",
      "Train time =  8682\n",
      "Test lasso loss = 1.6769\n",
      "MSE = 0.87313\n",
      "RMSE = 0.93442\n",
      "MAE = 0.74546\n",
      "Beta_diff = 0.9388\n"
     ]
    }
   ],
   "source": [
    "# Create model using best lambda\n",
    "print (\"Generating model\")\n",
    "best_lam = 0.01\n",
    "(B_model, train_time) = subgradient_descent(X_train, y_train, best_lam)\n",
    "\n",
    "# Run model on test set\n",
    "test_loss = loss(X_test, y_test, B_model, best_lam)\n",
    "y_pred = np.matmul(X_test, B_model)\n",
    "\n",
    "# Calc metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "beta_diff = LA.norm(B_model - B_true)\n",
    "\n",
    "print (\"\\n------------Results------------\")\n",
    "print (\"Train time = \", train_time)\n",
    "print (\"Test lasso loss = {0:.5}\".format(test_loss))\n",
    "print (\"MSE = {0:.5}\".format(mse))\n",
    "print (\"RMSE = {0:.5}\".format(rmse))\n",
    "print (\"MAE = {0:.5}\".format(mae))\n",
    "print (\"Beta_diff = {0:.5}\".format(beta_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating model for varying alpha\n",
      "original loss = 1130.9320357193367\n",
      "iterations for varying alpha done\n",
      "\n",
      "------------Results------------\n",
      "Train time =  206031\n",
      "Test lasso loss = 1.1682\n",
      "MSE = 0.040554\n",
      "RMSE = 0.20138\n",
      "MAE = 0.16085\n",
      "Beta_diff = 0.20217\n"
     ]
    }
   ],
   "source": [
    "#----------------- Variables -----------------\n",
    "beta_0 = np.random.multivariate_normal(np.ones(p), 2 * np.identity(p), size=1).reshape((p,1)) # initial beta\n",
    "best_lam = 0.01 # Found above in Part A\n",
    "MAX_ITERATIONS = 10000\n",
    "\n",
    "#----------------- Functions -----------------\n",
    "# Subgradient descent algo with varying alpha\n",
    "def subgradient_descent_varying_alpha(X, y, lam):\n",
    "    start = current_milli_time()\n",
    "    \n",
    "    alpha = 0.5\n",
    "    alpha_0 = alpha\n",
    "    beta = np.empty_like(beta_0)\n",
    "    beta[:] = beta_0\n",
    "    L = loss(X, y, beta, lam)\n",
    "    print (\"original loss = {}\".format(L))\n",
    "    L_min = [L,beta]\n",
    "    L2_store = []\n",
    "    c2_store = []\n",
    "    counter = 0\n",
    "    while counter < MAX_ITERATIONS:\n",
    "        counter += 1\n",
    "        beta_old = np.empty_like(beta)\n",
    "        beta_old[:] = beta\n",
    "        beta = beta - (alpha * subgrad(X, y, beta, lam))\n",
    "        delta_beta = norm(beta-beta_old) / p\n",
    "        alpha = alpha_0 / (alpha_0 + counter)\n",
    "        L = loss(X, y, beta, lam)\n",
    "        if L < L_min[0]:\n",
    "            L_min[0] = L\n",
    "            L_min[1] = beta\n",
    "        else:\n",
    "            beta = beta_old\n",
    "        if counter % 10 == 0:\n",
    "            L2_store.append(L)\n",
    "            c2_store.append(counter)\n",
    "#         if counter % 100 == 0:\n",
    "#             print (\"loss = {}\".format(L))\n",
    "        \n",
    "    print(\"iterations for varying alpha done\")\n",
    "    return (beta, current_milli_time() - start)\n",
    "\n",
    "# Train model using best lambda\n",
    "print (\"Generating model for varying alpha\")\n",
    "(B_model, train_time) = subgradient_descent_varying_alpha(X_train, y_train, best_lam)\n",
    "\n",
    "# Run model on test set\n",
    "test_loss = loss(X_test, y_test, B_model, best_lam)\n",
    "y_pred = np.matmul(X_test, B_model)\n",
    "\n",
    "# Calc metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "beta_diff = LA.norm(B_model - B_true)\n",
    "\n",
    "print (\"\\n------------Results------------\")\n",
    "print (\"Train time = \", train_time)\n",
    "print (\"Test lasso loss = {0:.5}\".format(test_loss))\n",
    "print (\"MSE = {0:.5}\".format(mse))\n",
    "print (\"RMSE = {0:.5}\".format(rmse))\n",
    "print (\"MAE = {0:.5}\".format(mae))\n",
    "print (\"Beta_diff = {0:.5}\".format(beta_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Proximal gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating model for proximal grad descent\n",
      "original loss = 1130.9320357193367\n",
      "\n",
      "------------Results------------\n",
      "Train time =  8695\n",
      "Test lasso loss = 1.6688\n",
      "MSE = 0.87679\n",
      "RMSE = 0.93637\n",
      "MAE = 0.74608\n",
      "Beta_diff = 0.93795\n"
     ]
    }
   ],
   "source": [
    "#----------------- Variables -----------------\n",
    "best_lam = 0.01 # Found above in Part A\n",
    "MAX_ITERATIONS = 10000\n",
    "TOLERANCE = 0.01\n",
    "\n",
    "#----------------- Functions -----------------\n",
    "def soft_thresh(lam, alpha, beta):\n",
    "    m, _ = np.shape(beta)\n",
    "    s_t = np.zeros((m,1))\n",
    "    for i in range(m):\n",
    "        if beta[i] > lam * alpha:\n",
    "            s_t[i,0] = beta[i] - lam * alpha\n",
    "        elif beta[i] < - lam * alpha:\n",
    "            s_t[i,0] = beta[i] + lam * alpha\n",
    "        else:\n",
    "            s_t[i,0] = 0\n",
    "    return s_t\n",
    "\n",
    "def g(beta, X, y):\n",
    "    # corresponds to the smooth term in the loss\n",
    "    N,_ = np.shape(X)\n",
    "    return   (1/N) * 0.5 * norm((np.matmul(X,beta)-y), ord=2)**2 \n",
    "\n",
    "def grad_g(beta, X, y):\n",
    "    # corresponds to the gradient of the smooth term in the loss\n",
    "    N,_ = np.shape(X)\n",
    "    return   (1/N) * np.matmul(np.transpose(X),(np.matmul(X,beta)-y)) \n",
    "\n",
    "# Proximal gradient descent algo\n",
    "def proximal_gradient_descent(X, y, lam):\n",
    "    start = current_milli_time()\n",
    "    \n",
    "    alpha = 0.01\n",
    "    beta = np.empty_like(beta_0)\n",
    "    beta[:] = beta_0\n",
    "    old_loss = sys.maxsize\n",
    "    L = loss(X, y, beta, lam)\n",
    "    print (\"original loss = {}\".format(L))\n",
    "    L3_store = []\n",
    "    c3_store = []\n",
    "    counter = 0\n",
    "    while counter < MAX_ITERATIONS and L < old_loss - TOLERANCE:\n",
    "        beta_old = np.empty_like(beta)\n",
    "        beta_old[:] = beta\n",
    "        counter += 1\n",
    "        beta = beta - alpha * grad_g(beta, X, y)\n",
    "        beta = soft_thresh(lam, alpha, beta)\n",
    "        old_loss = L\n",
    "        L = loss(X, y, beta, lam)\n",
    "        counter += 1\n",
    "        if counter % 10 == 0:\n",
    "            L3_store.append(L)\n",
    "        if counter % 100 == 0:\n",
    "            c3_store.append(counter)\n",
    "#             print (\"loss = {}\".format(L))\n",
    "            \n",
    "    return (beta, current_milli_time() - start)\n",
    "    \n",
    "# Train model using best lambda\n",
    "print (\"Generating model for proximal grad descent\")\n",
    "(B_model, train_time) = proximal_gradient_descent(X_train, y_train, best_lam)\n",
    "\n",
    "# Run model on test set\n",
    "test_loss = loss(X_test, y_test, B_model, best_lam)\n",
    "y_pred = np.matmul(X_test, B_model)\n",
    "\n",
    "# Calc metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "beta_diff = LA.norm(B_model - B_true)\n",
    "\n",
    "print (\"\\n------------Results------------\")\n",
    "print (\"Train time = \", train_time)\n",
    "print (\"Test lasso loss = {0:.5}\".format(test_loss))\n",
    "print (\"MSE = {0:.5}\".format(mse))\n",
    "print (\"RMSE = {0:.5}\".format(rmse))\n",
    "print (\"MAE = {0:.5}\".format(mae))\n",
    "print (\"Beta_diff = {0:.5}\".format(beta_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: Proximal gradient descent with backtracking line search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating model for proximal grad descent with backtracking\n",
      "original loss = 1130.9320357193367\n",
      "\n",
      "------------Results------------\n",
      "Train time =  1327\n",
      "Test lasso loss = 1.1451\n",
      "MSE = 0.0020986\n",
      "RMSE = 0.045811\n",
      "MAE = 0.036587\n",
      "Beta_diff = 0.046087\n"
     ]
    }
   ],
   "source": [
    "#----------------- Variables -----------------\n",
    "best_lam = 0.01 # Found above in Part A\n",
    "MAX_ITERATIONS = 10000\n",
    "TOLERANCE = 0.01\n",
    "\n",
    "#----------------- Functions -----------------\n",
    "\n",
    "# Proximal gradient descent with backtracking line search\n",
    "def proximal_gradient_descent_with_backtracking(X, y, lam):\n",
    "    start = current_milli_time()\n",
    "    \n",
    "    alpha = 10\n",
    "    multiplier = 0.9\n",
    "    # beta_0 = random.multivariate_normal(np.zeros(p), np.identity(p), size=1).reshape((p,1))\n",
    "    beta = np.empty_like(beta_0)\n",
    "    beta[:] = beta_0\n",
    "    L = loss(X, y, beta, lam)\n",
    "    old_loss = sys.maxsize\n",
    "    print (\"original loss = {}\".format(L))\n",
    "    L4_store = []\n",
    "    c4_store = []\n",
    "    counter = 0\n",
    "\n",
    "    while counter < MAX_ITERATIONS and L < old_loss - TOLERANCE:\n",
    "        beta_old = np.empty_like(beta)\n",
    "        beta_old[:] = beta\n",
    "\n",
    "        # while loop is backtracking line search to find optimal alpha\n",
    "        while True:\n",
    "            beta_plus = beta - alpha * grad_g(beta, X, y)\n",
    "            value = g(beta_plus, X, y) > g(beta, X, y) + np.matmul(np.transpose(grad_g(beta, X, y)), \n",
    "                    (beta_plus - beta)) + (1/(2*alpha)) * norm(beta_plus-beta)**2\n",
    "            if value:\n",
    "                alpha *= multiplier\n",
    "            else:\n",
    "                break\n",
    "#             print (\"loss = {}\".format(L))\n",
    "\n",
    "        counter += 1\n",
    "        beta = beta - alpha * grad_g(beta, X, y)\n",
    "        beta = soft_thresh(lam, alpha, beta) \n",
    "\n",
    "        old_loss = L\n",
    "        L = loss(X, y, beta, lam)\n",
    "        if counter % 1 == 0:\n",
    "            L4_store.append(L)\n",
    "#         if counter % 1 == 0:\n",
    "#             c4_store.append(counter)\n",
    "#             print (\"loss = {}\".format(L))\n",
    "#             print (\"alpha = {}\".format(alpha))\n",
    "#     #         print (\"beta diff: {}\".format(delta_beta))\n",
    "#             print ()\n",
    "    return (beta, current_milli_time() - start)\n",
    "\n",
    "\n",
    "# Train model using best lambda\n",
    "print (\"Generating model for proximal grad descent with backtracking\")\n",
    "(B_model, train_time) = proximal_gradient_descent_with_backtracking(X_train, y_train, best_lam)\n",
    "\n",
    "# Run model on test set\n",
    "test_loss = loss(X_test, y_test, B_model, best_lam)\n",
    "y_pred = np.matmul(X_test, B_model)\n",
    "\n",
    "# Calc metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "beta_diff = LA.norm(B_model - B_true)\n",
    "\n",
    "print (\"\\n------------Results------------\")\n",
    "print (\"Train time = \", train_time)\n",
    "print (\"Test lasso loss = {0:.5}\".format(test_loss))\n",
    "print (\"MSE = {0:.5}\".format(mse))\n",
    "print (\"RMSE = {0:.5}\".format(rmse))\n",
    "print (\"MAE = {0:.5}\".format(mae))\n",
    "print (\"Beta_diff = {0:.5}\".format(beta_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D: Compare against public software (scikitlearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:1094: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lambda from LassoCV =  0.009796455308071779\n",
      "\n",
      "------------Results------------\n",
      "Train time =  354\n",
      "Test lasso loss = 1.1451\n",
      "MSE = 0.0019672\n",
      "RMSE = 0.044353\n",
      "MAE = 0.035426\n",
      "Beta_diff = 0.044617\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "best_lam = 0.01 # Found above in Part A\n",
    "\n",
    "# Using lasso cross validation K rounds to find best lambda from skleanr package\n",
    "lcv_model = LassoCV(alphas=None, cv=CV_rounds, max_iter=10000, tol=0.00001)\n",
    "lcv_model.fit(X_train, y_train)\n",
    "best_sk_lam = lcv_model.alpha_ # Best alpha selected from CV\n",
    "\n",
    "print (\"Best lambda from LassoCV = \",best_sk_lam)\n",
    "\n",
    "# Create lasso model using best lam from our experiment\n",
    "start = current_milli_time()\n",
    "lasso_model = Lasso(alpha = best_lam, random_state=rand_state, max_iter=10000, tol=0.01)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "train_time = current_milli_time() - start\n",
    "\n",
    "y_pred = lasso_model.predict(X_test)\n",
    "B_model = lasso_model.coef_\n",
    "B_model = np.reshape(B_model, (-1, 1))\n",
    "test_loss = loss(X_test, y_test, B_model, best_lam)\n",
    "\n",
    "# Calc metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "beta_diff = LA.norm(B_model - B_true)\n",
    "\n",
    "print (\"\\n------------Results------------\")\n",
    "print (\"Train time = \", train_time)\n",
    "print (\"Test lasso loss = {0:.5}\".format(test_loss))\n",
    "print (\"MSE = {0:.5}\".format(mse))\n",
    "print (\"RMSE = {0:.5}\".format(rmse))\n",
    "print (\"MAE = {0:.5}\".format(mae))\n",
    "print (\"Beta_diff = {0:.5}\".format(beta_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Overall, after running throuhg 5-Fold CV for our given lambdas, we found that 0.01 was the best lambda. We double checked that this was accurate by running LassoCV from sklearn and found the best lambda was similar at 0.00979.\n",
    "\n",
    "Below are the results / metrics for each algorithm.\n",
    "\n",
    "NOTE: the training time for subgradient descent with varying alphas is a lot higher due to the termination condition simply being the max iterations.\n",
    "\n",
    "From the results you can see that Proximal gradient descent with backtracking performances the best out of the other algorithms in part A-C and performs almost as well as the sklearn Lasso implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Algo                                   </td><td>Train Time (ms)</td><td>Test Lasso Loss</td><td>MSE       </td><td>RMSE     </td><td>MAE     </td><td>Beta Diff from Truth</td></tr>\n",
       "<tr><td>Subgradient Descent fixed alpha        </td><td>8682           </td><td>1.6769         </td><td>0.87313   </td><td>0.93442  </td><td>0.74546 </td><td>0.9388              </td></tr>\n",
       "<tr><td>Subgradient Descent varying alpha      </td><td>206031         </td><td>1.1682         </td><td>0.040554  </td><td>0.20138  </td><td>0.16085 </td><td>0.20217             </td></tr>\n",
       "<tr><td>Proximal gradient decent               </td><td>8695           </td><td>1.6688         </td><td>0.87679   </td><td>0.93637  </td><td>0.74608 </td><td>0.93795             </td></tr>\n",
       "<tr><td>Proximal gradient decent w/backtracking</td><td>1327           </td><td>1.1451         </td><td>0.0020986 </td><td>0.0458911</td><td>0.036587</td><td>0.046087            </td></tr>\n",
       "<tr><td>Sklearn Lasso                          </td><td>354            </td><td>1.1451         </td><td>0.00196782</td><td>0.044353 </td><td>0.035426</td><td>0.044617            </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "table = [\n",
    "    [\"Algo\",\"Train Time (ms)\", \"Test Lasso Loss\", \"MSE\", \"RMSE\", \"MAE\", \"Beta Diff from Truth\"],\n",
    "    [\"Subgradient Descent fixed alpha\", 8682, 1.6769, 0.87313, 0.93442, 0.74546, 0.9388],\n",
    "    [\"Subgradient Descent varying alpha\", 206031, 1.1682, 0.040554, 0.20138, 0.16085, 0.20217],\n",
    "    [\"Proximal gradient decent\", 8695, 1.6688, 0.87679, 0.93637, 0.74608, 0.93795],\n",
    "    [\"Proximal gradient decent w/backtracking\", 1327, 1.1451, 0.0020986, 0.0458911, 0.036587, 0.046087],\n",
    "    [\"Sklearn Lasso\", 354, 1.1451, 0.00196782, 0.044353, 0.035426, 0.044617]\n",
    "]\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
