{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author: Farzan Memarian\n",
    "### Problems: Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "from sklearn.model_selection import train_test_split, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA GENERATION\n",
    "p = 500 # dimension of the data points\n",
    "N = 10**5 # number of training points \n",
    "X1_orig = random.multivariate_normal(np.zeros(p), np.identity(p), size=N)\n",
    "X2_orig = random.multivariate_normal(np.zeros(p), np.identity(p), size=N)\n",
    "X_orig = np.concatenate((X1_orig,X2_orig),axis=0)\n",
    "nnz = int(p/25) # number of non-zeros\n",
    "nz_nums = random.multivariate_normal(np.ones(nnz), np.identity(nnz), size=1)\n",
    "indexes = random.choice(range(p), size=nnz, replace=False, p=None)\n",
    "ground_beta = np.zeros((p,1))\n",
    "ground_beta[indexes,0] = nz_nums\n",
    "from numpy import matmul\n",
    "y_orig = matmul(X_orig,ground_beta)\n",
    "X_tr, X_test, y_tr, y_test = train_test_split(X_orig,y_orig,test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Implement subgradient descent algorithm with your choice of stepsize. Try a fixed stepsize and a decreasing stepsize. Plot the objective function with growing number of iterations for each algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "from pdb import set_trace\n",
    "\n",
    "# these parameters are shared between all parts\n",
    "thresh_beta = 10**(-5) # change in beta, norm(beta-beta_old) / p\n",
    "thresh_loss = 3 \n",
    "beta_0 = random.multivariate_normal(np.ones(p), 2 * np.identity(p), size=1).reshape((p,1))\n",
    "\n",
    "def loss(X, y, beta, lam):\n",
    "    N,_ = np.shape(X)\n",
    "    return   (1/N) * 0.5 * norm((matmul(X,beta)-y), ord=2)**2  + lam * norm(beta, ord=1) \n",
    "\n",
    "def subgrad(X, y, beta, lam):\n",
    "    # gradient of the loss function\n",
    "    N,_ = np.shape(X)\n",
    "    return   (1/N) * matmul(np.transpose(X),(matmul(X,beta)-y)) + lam * np.sign(beta) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold counter: 1, lambda counter: 0\n",
      "<function loss at 0x7f30045f2ea0>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d34c1be1da87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mbeta_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mbeta_old\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mbeta\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msubgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mdelta_beta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbeta_old\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-ed9fb67dd22e>\u001b[0m in \u001b[0;36msubgrad\u001b[0;34m(X, y, beta, lam)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# gradient of the loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mreturn\u001b[0m   \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# FIXED ALPHA\n",
    "\n",
    "# TRAINING\n",
    "lams = [1,0.5,0.1]\n",
    "alpha = 0.001\n",
    "nsplits = 3\n",
    "kf = KFold(n_splits=nsplits)\n",
    "val_err = np.zeros(np.size(lams))\n",
    "# cross validation to find the best lambda\n",
    "fold_counter = 0\n",
    "for train_index, test_index in kf.split(X_tr):\n",
    "    fold_counter += 1\n",
    "    X, X_val = X_tr[train_index], X_tr[test_index]\n",
    "    y, y_val = y_tr[train_index], y_tr[test_index]\n",
    "    \n",
    "    for lam_counter, lam in enumerate(lams):\n",
    "        print (\"fold counter: {}, lambda counter: {}\".format(fold_counter, lam_counter))\n",
    "        beta = np.empty_like(beta_0)\n",
    "        beta[:] = beta_0\n",
    "\n",
    "        L = loss(X, y, beta, lam)\n",
    "        L_min = [L,beta]\n",
    "        print (loss)\n",
    "        counter = 0\n",
    "        while L>thresh_loss:\n",
    "            beta_old = np.empty_like(beta)\n",
    "            beta_old[:] = beta\n",
    "            beta -= alpha * subgrad(X, y, beta, lam)\n",
    "            delta_beta = norm(beta-beta_old) / p\n",
    "            L = loss(X, y, beta, lam)\n",
    "            if L < L_min[0]:\n",
    "                L_min[0] = L\n",
    "                L_min[1] = beta\n",
    "            else:\n",
    "                beta = beta_old\n",
    "            counter += 1\n",
    "            if counter % 100 == 0:\n",
    "                print (\"loss = {}\".format(L))\n",
    "\n",
    "        val_err[lam_counter] += loss(X_val, y_val, beta, lam)/nsplits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function loss at 0x7f30045f2ea0>\n",
      "loss = 450.5852886766054\n",
      "loss = 368.86613407668636\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-267117f35970>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mbeta\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msubgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mdelta_beta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbeta_old\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mL_min\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mL_min\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-ed9fb67dd22e>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(X, y, beta, lam)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mreturn\u001b[0m   \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# using best lambda\n",
    "lam = 0.1\n",
    "alpha = 0.001\n",
    "beta = np.empty_like(beta_0)\n",
    "beta[:] = beta_0\n",
    "L = loss(X, y, beta, lam)\n",
    "print (\"original loss = {}\".format(L))\n",
    "L_min = [L,beta]\n",
    "L1_store = []\n",
    "c1_store = []\n",
    "print (loss)\n",
    "counter = 0\n",
    "while L>thresh_loss:\n",
    "    beta_old = np.empty_like(beta)\n",
    "    beta_old[:] = beta\n",
    "    beta -= alpha * subgrad(X, y, beta, lam)\n",
    "    delta_beta = norm(beta-beta_old) / p\n",
    "    L = loss(X, y, beta, lam)\n",
    "    if L < L_min[0]:\n",
    "        L_min[0] = L\n",
    "        L_min[1] = beta\n",
    "    else:\n",
    "        beta = beta_old\n",
    "    counter += 1\n",
    "    if counter % 10 == 0:\n",
    "        L1_store.append(L)\n",
    "        c1_store.append(counter)\n",
    "    if counter % 100 ==0:\n",
    "        print (\"loss = {}\".format(L))\n",
    "q3a_fixed_beta = beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 522.9794511338991\n",
      "loss = 515.4626054672633\n",
      "loss = 511.18265703887954\n",
      "loss = 508.18570039272254\n",
      "loss = 505.880715836605\n",
      "loss = 504.0090341114262\n",
      "loss = 502.4341819671796\n",
      "loss = 501.0753644203938\n",
      "loss = 499.8807938049809\n",
      "loss = 498.8152889901862\n",
      "loss = 497.8538618715877\n",
      "loss = 496.97812887246573\n",
      "loss = 496.17417322529644\n",
      "loss = 495.43120694750405\n",
      "loss = 494.7406985691887\n",
      "loss = 494.09578609165305\n",
      "loss = 493.49086804550336\n",
      "loss = 492.9213131316738\n",
      "loss = 492.38324874998125\n",
      "loss = 491.8734039792504\n",
      "loss = 491.3889905344708\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-61986e2699d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mdelta_beta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbeta_old\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha_0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0malpha_0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mL_min\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mL_min\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-ed9fb67dd22e>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(X, y, beta, lam)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mreturn\u001b[0m   \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# DECREASING ALPHA\n",
    "\n",
    "# TRAINING\n",
    "X = X_tr\n",
    "y= y_tr\n",
    "lam = 0.1\n",
    "alpha_0 = 0.01\n",
    "beta = np.empty_like(beta_0)\n",
    "beta[:] = beta_0\n",
    "L = loss(X, y, beta, lam)\n",
    "print (\"original loss = {}\".format(L))\n",
    "L_min = [L,beta]\n",
    "L2_store = []\n",
    "c2_store = []\n",
    "counter = 0\n",
    "while L>thresh_loss:\n",
    "    counter += 1\n",
    "    beta_old = np.empty_like(beta)\n",
    "    beta_old[:] = beta\n",
    "    beta -= alpha * subgrad(X, y, beta, lam)\n",
    "    delta_beta = norm(beta-beta_old) / p\n",
    "    alpha = alpha_0 / (alpha_0 + counter)\n",
    "    L = loss(X, y, beta, lam)\n",
    "    if L < L_min[0]:\n",
    "        L_min[0] = L\n",
    "        L_min[1] = beta\n",
    "    else:\n",
    "        beta = beta_old\n",
    "    if counter % 10 == 0:\n",
    "        L2_store.append(L)\n",
    "        c2_store.append(counter)\n",
    "        print (\"loss = {}\".format(L))\n",
    "\n",
    "print(\"iterations for varying alpha done\")\n",
    "q3a_varying_beta = beta   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Implement proximal gradient descent with your choice of stepsize. Plot the objective function with growing number of iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_thresh(lam, alpha, beta):\n",
    "    m, _ = np.shape(beta)\n",
    "    s_t = np.zeros((m,1))\n",
    "    for i in range(m):\n",
    "        if beta[i] > lam * alpha:\n",
    "            s_t[i,0] = beta[i] - lam * alpha\n",
    "        elif beta[i] < - lam * alpha:\n",
    "            s_t[i,0] = beta[i] + lam * alpha\n",
    "        else:\n",
    "            s_t[i,0] = 0\n",
    "    return s_t\n",
    "\n",
    "def g(beta, X, y):\n",
    "    # corresponds to the smooth term in the loss\n",
    "    N,_ = np.shape(X)\n",
    "    return   (1/N) * 0.5 * norm((matmul(X,beta)-y), ord=2)**2 \n",
    "\n",
    "def grad_g(beta, X, y):\n",
    "    # corresponds to the gradient of the smooth term in the loss\n",
    "    N,_ = np.shape(X)\n",
    "    return   (1/N) * matmul(np.transpose(X),(matmul(X,beta)-y)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original loss = 892.5252222617148\n",
      "loss = 324.11206306189865\n",
      "loss = 118.24524480708021\n",
      "loss = 43.38126011732554\n",
      "loss = 16.224630476050493\n",
      "loss = 6.492466279016891\n",
      "loss = 3.237630919886187\n"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "X = X_tr\n",
    "y = y_tr\n",
    "lam = 0.1\n",
    "alpha = 0.01\n",
    "beta = np.empty_like(beta_0)\n",
    "beta[:] = beta_0\n",
    "L = loss(X, y, beta, lam)\n",
    "print (\"original loss = {}\".format(L))\n",
    "L3_store = []\n",
    "c3_store = []\n",
    "counter = 0\n",
    "while L>thresh_loss:\n",
    "    beta_old = np.empty_like(beta)\n",
    "    beta_old[:] = beta\n",
    "    counter += 1\n",
    "    beta = beta - alpha * grad_g(beta, X, y)\n",
    "    beta = soft_thresh(lam, alpha, beta)\n",
    "    L = loss(X, y, beta, lam)\n",
    "    counter += 1\n",
    "    if counter % 10 == 0:\n",
    "        L3_store.append(L)\n",
    "    if counter % 100 == 0:\n",
    "        c3_store.append(counter)\n",
    "        print (\"loss = {}\".format(L))\n",
    "q3b_beta = beta    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Implement proximal gradient descent with backtracking line search. You can find more about backtracking line search in https://www.robots.ox.ac. uk/~vgg/rg/slides/fgrad.pdf. Plot the objective function with growing number of iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original loss = 892.5252222617148\n",
      "loss = 4.6623214931045585\n",
      "alpha = 0.9847709021836112\n",
      "\n",
      "loss = 2.29414060880034\n",
      "alpha = 0.9847709021836112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "X = X_tr\n",
    "y = y_tr\n",
    "lam = 0.1\n",
    "alpha = 10\n",
    "multiplier = 0.9\n",
    "# beta_0 = random.multivariate_normal(np.zeros(p), np.identity(p), size=1).reshape((p,1))\n",
    "beta = np.empty_like(beta_0)\n",
    "beta[:] = beta_0\n",
    "L = loss(X, y, beta, lam)\n",
    "print (\"original loss = {}\".format(L))\n",
    "L4_store = []\n",
    "c4_store = []\n",
    "counter = 0\n",
    "\n",
    "while L>thresh_loss:\n",
    "    beta_old = np.empty_like(beta)\n",
    "    beta_old[:] = beta\n",
    "    \n",
    "    # while loop is backtracking line search to find optimal alpha\n",
    "    while True:\n",
    "        beta_plus = beta - alpha * grad_g(beta, X, y)\n",
    "        value = g(beta_plus, X, y) > g(beta, X, y) + matmul(np.transpose(grad_g(beta, X, y)), \n",
    "                (beta_plus - beta)) + (1/(2*alpha)) * norm(beta_plus-beta)**2\n",
    "        if value:\n",
    "            alpha *= multiplier\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    counter += 1\n",
    "    beta = beta - alpha * grad_g(beta, X, y)\n",
    "    beta = soft_thresh(lam, alpha, beta) \n",
    "    \n",
    "    L = loss(X, y, beta, lam)\n",
    "    if counter % 1 == 0:\n",
    "        L4_store.append(L)\n",
    "    if counter % 1 == 0:\n",
    "        c4_store.append(counter)\n",
    "        print (\"loss = {}\".format(L))\n",
    "        print (\"alpha = {}\".format(alpha))\n",
    "#         print (\"beta diff: {}\".format(delta_beta))\n",
    "        print ()\n",
    "q3c_beta = beta     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.39425435695 2.39425435695\n"
     ]
    }
   ],
   "source": [
    "L_tr = loss(X_tr, y_tr, ground_beta, lam)\n",
    "L_test = loss(X_test, y_test, ground_beta, lam)\n",
    "print(L_tr, L_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015338492622777684"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_diff = norm(beta-ground_beta, ord=1)/p\n",
    "beta_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Now compare these methods with any publicly available software for lasso, e.g. glmfit or lasso or scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "clf = Lasso(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, \n",
    "                           copy_X=True, max_iter=1000, tol=0.001, warm_start=False, positive=False, \n",
    "                           random_state=None, selection=’cyclic’)\n",
    "clf.fit(X,y)\n",
    "y_pred = clf.predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
