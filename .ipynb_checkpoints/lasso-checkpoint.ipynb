{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA GENERATION\n",
    "p = 500\n",
    "N = 10**5\n",
    "cov = 2 * np.identity(p)\n",
    "mean = np.ones(p)\n",
    "X = random.multivariate_normal(mean, cov, size=N)\n",
    "nnz = int(p/25)\n",
    "nz_nums = random.multivariate_normal(np.ones(nnz), np.identity(nnz), size=1)\n",
    "indexes = random.choice(range(p), size=nnz, replace=False, p=None)\n",
    "ground_beta = np.zeros((p,1))\n",
    "ground_beta[indexes,0] = nz_nums\n",
    "from numpy import matmul\n",
    "y = matmul(X,ground_beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Implement subgradient descent algorithm with your choice of stepsize. Try a fixed stepsize and a decreasing stepsize. Plot the objective function with growing number of iterations for each algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function loss at 0x7f174c05bd90>\n",
      "loss = 489.96987869909344\n",
      "loss = 470.6605079821329\n",
      "loss = 452.11597006252373\n",
      "loss = 434.3056043264041\n",
      "loss = 417.2002070009529\n",
      "loss = 400.7717412679746\n",
      "loss = 384.9932906952771\n",
      "loss = 369.8390141425748\n",
      "loss = 355.2841026180022\n",
      "loss = 341.3047378549783\n",
      "loss = 327.87805257071153\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-8f1fa741bea4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mbeta_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mbeta\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mL_min\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-8f1fa741bea4>\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(X, y, beta, lam)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mreturn\u001b[0m   \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from numpy.linalg import norm\n",
    "from pdb import set_trace\n",
    "\n",
    "lam = 0.1 # this is lambda\n",
    "thresh = 0.1\n",
    "mean = np.zeros(p)\n",
    "cov = np.identity(p)\n",
    "beta_0 = random.multivariate_normal(mean, cov, size=1).reshape((p,1))\n",
    "\n",
    "def loss(X, y, beta, lam):\n",
    "#     set_trace()\n",
    "    return   (1/N) * ( 0.5 * norm((matmul(X,beta)-y), ord=2)**2  + lam * norm(beta, ord=1) )\n",
    "\n",
    "def grad(X, y, beta, lam):\n",
    "    return   (1/N) * ( matmul(np.transpose(X),(matmul(X,beta)-y)) + lam * np.sign(beta) )\n",
    "\n",
    "\n",
    "\n",
    "# FIXED ALPHA\n",
    "alpha = 0.001\n",
    "beta = np.empty_like(beta_0)\n",
    "beta[:] = beta_0\n",
    "L = loss(X, y, beta, lam)\n",
    "L_min = [L,beta]\n",
    "L1_store = []\n",
    "c1_store = []\n",
    "print (loss)\n",
    "counter = 0\n",
    "while L > thresh:\n",
    "    beta_old = beta\n",
    "    beta -= alpha * grad(X, y, beta, lam)\n",
    "    L = loss(X, y, beta, lam)\n",
    "    if L < L_min[0]:\n",
    "        L_min[0] = L\n",
    "        L_min[1] = beta\n",
    "    else:\n",
    "        beta = beta_old\n",
    "    counter += 1\n",
    "    if counter % 10 == 0:\n",
    "        L1_store.append(L)\n",
    "        c1_store.append(counter)\n",
    "        print (\"loss = {}\".format(L))\n",
    "print(\"iterations for fixed alpha done\") \n",
    "        \n",
    "# # DECREASING ALPHA\n",
    "# alpha_0 = 0.01\n",
    "# beta = np.empty_like(beta_0)\n",
    "# beta[:] = beta_0\n",
    "# L = loss(X, y, beta, lam)\n",
    "# L_min = [L,beta]\n",
    "# L2_store = []\n",
    "# c2_store = []\n",
    "# counter = 0\n",
    "# while L > thresh:\n",
    "#     counter += 1\n",
    "#     beta_old = beta\n",
    "#     beta -= alpha * grad(X, y, beta, lam)\n",
    "#     alpha = alpha_0 / (alpha_0 + counter)\n",
    "#     L = loss(X, y, beta, lam)\n",
    "#     if L < L_min[0]:\n",
    "#         L_min[0] = L\n",
    "#         L_min[1] = beta\n",
    "#     else:\n",
    "#         beta = beta_old\n",
    "#     if counter % 10 == 0:\n",
    "#         L2_store.append(L)\n",
    "#         c2_store.append(counter)\n",
    "#         print (\"loss = {}\".format(L))\n",
    "# print(\"iterations for varying alpha done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Implement proximal gradient descent with your choice of stepsize. Plot the objective function with growing number of iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 537.8791874590434\n",
      "loss = 526.9064807045226\n",
      "loss = 516.404141335407\n",
      "loss = 506.1120345274667\n",
      "loss = 496.0256991714446\n",
      "loss = 486.1410010683938\n",
      "loss = 476.4538896967428\n",
      "loss = 466.968387581976\n",
      "loss = 457.67352085710087\n",
      "loss = 448.564617303455\n",
      "loss = 439.63775559783306\n",
      "loss = 430.88924150665747\n",
      "loss = 422.31549454945235\n",
      "loss = 413.9132691991406\n",
      "loss = 405.6789253581327\n",
      "loss = 397.60902207453546\n",
      "loss = 389.70025947734683\n",
      "loss = 381.94951080519155\n",
      "loss = 374.35372268221823\n",
      "loss = 366.9096329859101\n",
      "loss = 359.61412112415815\n",
      "loss = 352.4642071754867\n",
      "loss = 345.45697127333165\n",
      "loss = 338.58964970322\n",
      "loss = 331.859397825041\n",
      "loss = 325.2634070146795\n",
      "loss = 318.7989858773664\n",
      "loss = 312.46349723109716\n",
      "loss = 306.25435700851324\n",
      "loss = 300.1690331820448\n",
      "loss = 294.2050570027377\n",
      "loss = 288.36165547480437\n",
      "loss = 282.6359408570029\n",
      "loss = 277.02435380978744\n",
      "loss = 271.5269897746475\n",
      "loss = 266.14307663676436\n",
      "loss = 260.86644438937293\n",
      "loss = 255.69591776926399\n",
      "loss = 250.63258821533117\n",
      "loss = 245.67014566831486\n",
      "loss = 240.80675401947096\n",
      "loss = 236.04060909360135\n",
      "loss = 231.36951789526645\n",
      "loss = 226.7917066014452\n",
      "loss = 222.30522481377076\n",
      "loss = 217.9080397076495\n",
      "loss = 213.59836532543605\n",
      "loss = 209.37446403433725\n",
      "loss = 205.2346112889175\n",
      "loss = 201.17712404898998\n",
      "loss = 197.2005045142782\n",
      "loss = 193.3029915394358\n",
      "loss = 189.4829968724667\n",
      "loss = 185.7389763180183\n",
      "loss = 182.0697457450251\n",
      "loss = 178.47381592430463\n",
      "loss = 174.94936494602894\n",
      "loss = 171.49499437792304\n",
      "loss = 168.10956363911927\n",
      "loss = 164.79156758375453\n",
      "loss = 161.5395630184277\n",
      "loss = 158.352165157418\n",
      "loss = 155.22808061278334\n",
      "loss = 152.16604514049052\n",
      "loss = 149.1648203977945\n",
      "loss = 146.2232645971779\n",
      "loss = 143.34013356638118\n",
      "loss = 140.51426750452907\n",
      "loss = 137.74457455569362\n",
      "loss = 135.03006466325456\n",
      "loss = 132.369438692831\n",
      "loss = 129.76163399677534\n",
      "loss = 127.20564367823462\n",
      "loss = 124.70036245938856\n",
      "loss = 122.24477945507388\n",
      "loss = 119.83790403713095\n",
      "loss = 117.47876541834388\n",
      "loss = 115.16653109252982\n",
      "loss = 112.9003338170002\n",
      "loss = 110.67922882083505\n",
      "loss = 108.50215056604752\n",
      "loss = 106.36833673471577\n",
      "loss = 104.2769518396791\n",
      "loss = 102.22701088941136\n",
      "loss = 100.21877451705303\n",
      "loss = 98.25274775636684\n",
      "loss = 96.32569036947409\n",
      "loss = 94.43687567967073\n",
      "loss = 92.58552630219171\n",
      "loss = 90.77086056086647\n",
      "loss = 88.99269741136077\n",
      "loss = 87.2531351263239\n",
      "loss = 85.54800887522602\n",
      "loss = 83.87663363422335\n",
      "loss = 82.23833819677901\n",
      "loss = 80.63248176737984\n",
      "loss = 79.05843366054057\n",
      "loss = 77.51569080913559\n",
      "loss = 76.00352757154205\n",
      "loss = 74.5212757536631\n",
      "loss = 73.06834126747721\n",
      "loss = 71.6441706205737\n",
      "loss = 70.24821638529652\n",
      "loss = 68.88001974292597\n",
      "loss = 67.53912116256437\n",
      "loss = 66.22480680389918\n",
      "loss = 64.93646379037071\n",
      "loss = 63.67357653752565\n",
      "loss = 62.435800923444525\n",
      "loss = 61.222599339125615\n",
      "loss = 60.03335675251486\n",
      "loss = 58.8676007069039\n",
      "loss = 57.72486701287447\n",
      "loss = 56.604694890480765\n",
      "loss = 55.50676177884768\n",
      "loss = 54.4305519137202\n",
      "loss = 53.375612946724345\n",
      "loss = 52.341568970990075\n",
      "loss = 51.3279634311727\n",
      "loss = 50.334373168563104\n",
      "loss = 49.36042329373352\n",
      "loss = 48.40584499229041\n",
      "loss = 47.47016326029851\n",
      "loss = 46.552961073555444\n",
      "loss = 45.65388247600205\n",
      "loss = 44.77261888797853\n",
      "loss = 43.908756316100664\n",
      "loss = 43.063295577582245\n",
      "loss = 42.23554748564798\n",
      "loss = 41.42411360353927\n",
      "loss = 40.62870050137742\n",
      "loss = 39.849086930772245\n",
      "loss = 39.084836389194564\n",
      "loss = 38.335721033720276\n",
      "loss = 37.601423186506224\n",
      "loss = 36.88160653812224\n",
      "loss = 36.17603796182656\n",
      "loss = 35.48456698354319\n",
      "loss = 34.80677440763119\n",
      "loss = 34.14240438477458\n",
      "loss = 33.49120482855296\n",
      "loss = 32.852983521120834\n",
      "loss = 32.22735153965662\n",
      "loss = 31.614033912222506\n",
      "loss = 31.012924260884795\n",
      "loss = 30.423749581215468\n",
      "loss = 29.84617591751127\n",
      "loss = 29.279975283350826\n",
      "loss = 28.72491199602033\n",
      "loss = 28.18077368226089\n",
      "loss = 27.647371087849855\n",
      "loss = 27.12456327899268\n",
      "loss = 26.612035928114885\n",
      "loss = 26.109596241538924\n",
      "loss = 25.617065137337296\n",
      "loss = 25.13425422179011\n",
      "loss = 24.66096115859795\n",
      "loss = 24.197010777917086\n",
      "loss = 23.742243793119524\n",
      "loss = 23.296463560866034\n",
      "loss = 22.859534477834465\n",
      "loss = 22.43122295290308\n",
      "loss = 22.011326926357\n",
      "loss = 21.5997875734595\n",
      "loss = 21.19649325576013\n",
      "loss = 20.8012015319919\n",
      "loss = 20.413729600832795\n",
      "loss = 20.033866476551832\n",
      "loss = 19.66151636478545\n",
      "loss = 19.296488318096618\n",
      "loss = 18.938662239088625\n",
      "loss = 18.587959858396026\n",
      "loss = 18.2441499048056\n",
      "loss = 17.90716828095406\n",
      "loss = 17.576870760645125\n",
      "loss = 17.253117534040026\n",
      "loss = 16.935760482027582\n",
      "loss = 16.624632232354056\n",
      "loss = 16.319653566158586\n",
      "loss = 16.02073835702217\n",
      "loss = 15.727715869968172\n",
      "loss = 15.440464438184868\n",
      "loss = 15.15893168476312\n",
      "loss = 14.88305264867471\n",
      "loss = 14.612594510753418\n",
      "loss = 14.347440745234893\n",
      "loss = 14.08752689120797\n",
      "loss = 13.832852270516288\n",
      "loss = 13.583269332034305\n",
      "loss = 13.33865300639664\n",
      "loss = 13.098851871012517\n",
      "loss = 12.863752325967422\n",
      "loss = 12.633360356474995\n",
      "loss = 12.407572120938108\n",
      "loss = 12.186341337687558\n",
      "loss = 11.96950974155434\n",
      "loss = 11.757017868173548\n",
      "loss = 11.548768159320309\n",
      "loss = 11.344653847936423\n",
      "loss = 11.144549771828292\n",
      "loss = 10.9484028267299\n",
      "loss = 10.756219245444449\n",
      "loss = 10.56787334651759\n",
      "loss = 10.383338238292282\n",
      "loss = 10.202430757659302\n",
      "loss = 10.025063142418936\n",
      "loss = 9.851196845684932\n",
      "loss = 9.680738704075432\n",
      "loss = 9.513727652091458\n",
      "loss = 9.350146841883323\n",
      "loss = 9.189803345852226\n",
      "loss = 9.03262896252117\n",
      "loss = 8.878605970778349\n",
      "loss = 8.727605252986162\n",
      "loss = 8.579604709389354\n",
      "loss = 8.434497943406782\n",
      "loss = 8.292249865811153\n",
      "loss = 8.15281423210065\n",
      "loss = 8.016224841212223\n",
      "loss = 7.882399073977968\n",
      "loss = 7.7511887514349604\n",
      "loss = 7.622633596344954\n",
      "loss = 7.496720582412113\n",
      "loss = 7.373433560088983\n",
      "loss = 7.252599952203457\n",
      "loss = 7.13421231393362\n",
      "loss = 7.018267857526444\n",
      "loss = 6.904744874533208\n",
      "loss = 6.7936541169042854\n",
      "loss = 6.684867138989395\n",
      "loss = 6.578324393783638\n",
      "loss = 6.473937543998566\n",
      "loss = 6.37168889570874\n",
      "loss = 6.271469878384492\n",
      "loss = 6.17331092348412\n",
      "loss = 6.077144314803796\n",
      "loss = 5.9829390436034515\n",
      "loss = 5.8906462449138735\n",
      "loss = 5.80016213287767\n",
      "loss = 5.711525342782185\n",
      "loss = 5.624808269906336\n",
      "loss = 5.539853807511207\n",
      "loss = 5.456655235512567\n",
      "loss = 5.375182903166044\n",
      "loss = 5.295386997560948\n",
      "loss = 5.217159533348356\n",
      "loss = 5.140603855870705\n",
      "loss = 5.0656321255605725\n",
      "loss = 4.992178623656167\n",
      "loss = 4.920161978347465\n",
      "loss = 4.849622059981492\n",
      "loss = 4.78059033907341\n",
      "loss = 4.712982313619797\n",
      "loss = 4.646841879920652\n",
      "loss = 4.582102149024373\n",
      "loss = 4.518738113500425\n",
      "loss = 4.45668328398407\n",
      "loss = 4.3958529669669595\n",
      "loss = 4.336244225749708\n",
      "loss = 4.277923386168877\n",
      "loss = 4.220790151035379\n",
      "loss = 4.164954464293186\n",
      "loss = 4.110319137847651\n",
      "loss = 4.056858588864633\n",
      "loss = 4.0046651683957775\n",
      "loss = 3.9536956569651935\n",
      "loss = 3.903910174536335\n",
      "loss = 3.855241446903906\n",
      "loss = 3.8077460117623385\n",
      "loss = 3.7613847087907035\n",
      "loss = 3.7159880998672\n",
      "loss = 3.6714824166948996\n",
      "loss = 3.627995910305798\n",
      "loss = 3.585411453485146\n",
      "loss = 3.5437892872044747\n",
      "loss = 3.5030393755899696\n",
      "loss = 3.463117313493325\n",
      "loss = 3.4241002700860848\n",
      "loss = 3.3859019786801117\n",
      "loss = 3.3486328956610962\n",
      "loss = 3.3122347973335584\n",
      "loss = 3.2766429425365624\n",
      "loss = 3.2418783669053406\n",
      "loss = 3.207866763668381\n",
      "loss = 3.1746393804156154\n",
      "loss = 3.142303539798602\n",
      "loss = 3.1108080695743805\n",
      "loss = 3.079952317680145\n",
      "loss = 3.049696476338183\n",
      "loss = 3.0201588952536107\n",
      "loss = 2.9913119851528993\n",
      "loss = 2.963112029355565\n",
      "loss = 2.9356300034978755\n",
      "loss = 2.9088759746603086\n",
      "loss = 2.8828922041429106\n",
      "loss = 2.857626449049697\n",
      "loss = 2.8330378944348897\n",
      "loss = 2.808987754913716\n",
      "loss = 2.785647734700991\n",
      "loss = 2.762792414714852\n",
      "loss = 2.7404449725240947\n",
      "loss = 2.71883967389731\n",
      "loss = 2.697758373174178\n",
      "loss = 2.677275573506202\n",
      "loss = 2.65723676958753\n",
      "loss = 2.637699722089266\n",
      "loss = 2.6186223065677092\n",
      "loss = 2.5999243382496697\n",
      "loss = 2.581673793400015\n",
      "loss = 2.5639156450180653\n",
      "loss = 2.5465270864203995\n",
      "loss = 2.529508755977697\n",
      "loss = 2.512987577857741\n",
      "loss = 2.497026532259284\n",
      "loss = 2.481558310350105\n",
      "loss = 2.4664448906997003\n",
      "loss = 2.4516451959025614\n",
      "loss = 2.4371568372376324\n",
      "loss = 2.4230494677341405\n",
      "loss = 2.4093649927179355\n",
      "loss = 2.39602803201262\n",
      "loss = 2.3829572947285564\n",
      "loss = 2.3701533516855267\n",
      "loss = 2.357639990912653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 2.3453949117298154\n",
      "loss = 2.3335976319128426\n",
      "loss = 2.3223271439288453\n",
      "loss = 2.311434290989912\n",
      "loss = 2.3008836442113125\n",
      "loss = 2.2905416546082407\n",
      "loss = 2.280418290160615\n",
      "loss = 2.2705504751083967\n",
      "loss = 2.2609742772567425\n",
      "loss = 2.2516496158979655\n",
      "loss = 2.242949526295558\n",
      "loss = 2.234526236723461\n",
      "loss = 2.226506294335726\n",
      "loss = 2.218755392168681\n",
      "loss = 2.2111700933045237\n",
      "loss = 2.203895408805457\n",
      "loss = 2.196838555233975\n",
      "loss = 2.190123189612139\n",
      "loss = 2.183565626458183\n",
      "loss = 2.1773598251626423\n",
      "loss = 2.1714041620300333\n",
      "loss = 2.1657191931711317\n",
      "loss = 2.1603967619143396\n",
      "loss = 2.1553090777324724\n",
      "loss = 2.1503981032058417\n",
      "loss = 2.1458261462412906\n",
      "loss = 2.1414990427940435\n",
      "loss = 2.1373984544908686\n",
      "loss = 2.133383593377061\n",
      "loss = 2.1294667894428123\n",
      "loss = 2.125770374799241\n",
      "loss = 2.122299097086771\n",
      "loss = 2.1190420097279112\n",
      "loss = 2.115866357061023\n",
      "loss = 2.1127584345178505\n",
      "loss = 2.109712200107983\n",
      "loss = 2.1067261183771433\n",
      "loss = 2.103798877609557\n",
      "loss = 2.100929262946775\n",
      "loss = 2.0982063771706234\n",
      "loss = 2.0956050088744456\n",
      "loss = 2.093057451927676\n",
      "loss = 2.090561822310945\n",
      "loss = 2.0881831620086273\n",
      "loss = 2.086000972901321\n",
      "loss = 2.08398353143069\n",
      "loss = 2.0820079428921074\n",
      "loss = 2.0800722544916987\n",
      "loss = 2.0781751514730376\n",
      "loss = 2.0763156259128097\n",
      "loss = 2.0744928213797116\n",
      "loss = 2.0727059599970534\n",
      "loss = 2.070954308118607\n",
      "loss = 2.0693293756052142\n",
      "loss = 2.067816873171701\n",
      "loss = 2.066334968114796\n",
      "loss = 2.064882691584568\n",
      "loss = 2.063459285253634\n",
      "loss = 2.062064096431552\n",
      "loss = 2.0606969691568624\n",
      "loss = 2.0593569124140547\n",
      "loss = 2.0580433541936736\n",
      "loss = 2.0567559706043963\n",
      "loss = 2.0554942981941107\n",
      "loss = 2.054257572276344\n",
      "loss = 2.053045299043373\n",
      "loss = 2.0518569946171437\n",
      "loss = 2.050692184772239\n",
      "loss = 2.049550404705838\n",
      "loss = 2.048431198832305\n",
      "loss = 2.0473341205915605\n",
      "loss = 2.046258901818971\n",
      "loss = 2.0452051174853483\n",
      "loss = 2.044172167984769\n",
      "loss = 2.0431596404746393\n",
      "loss = 2.0421671306491485\n",
      "loss = 2.041194242383127\n",
      "loss = 2.0402410820146195\n",
      "loss = 2.0393067793646864\n",
      "loss = 2.038390949021652\n",
      "loss = 2.0374932836588875\n",
      "loss = 2.036613791745154\n",
      "loss = 2.035751690503235\n",
      "loss = 2.034906633901629\n",
      "loss = 2.0340782837854867\n",
      "loss = 2.033266309205799\n",
      "loss = 2.0324703860195266\n",
      "loss = 2.0316901966263723\n",
      "loss = 2.0309254297750545\n",
      "loss = 2.0301757804055467\n",
      "loss = 2.0294409495104757\n",
      "loss = 2.0287206440072914\n",
      "loss = 2.028014576616963\n",
      "loss = 2.027322465747085\n",
      "loss = 2.026644035378295\n",
      "loss = 2.025979014953439\n",
      "loss = 2.0253271392691836\n",
      "loss = 2.0246883749969555\n",
      "loss = 2.0240624250882777\n",
      "loss = 2.02344902316515\n",
      "loss = 2.022848260043116\n",
      "loss = 2.0222594075605476\n",
      "loss = 2.021682199852082\n",
      "loss = 2.021116403915763\n",
      "loss = 2.0205618954467846\n",
      "loss = 2.020018675048744\n",
      "loss = 2.0194861959716333\n",
      "loss = 2.0189642437383974\n",
      "loss = 2.018452609102889\n",
      "loss = 2.017951087473645\n",
      "loss = 2.0174599843218592\n",
      "loss = 2.0169790112327184\n",
      "loss = 2.016507575200671\n",
      "loss = 2.016045462629809\n",
      "loss = 2.0155924866364896\n",
      "loss = 2.0151484652085654\n",
      "loss = 2.0147132205602096\n",
      "loss = 2.0142865787559994\n",
      "loss = 2.013868369479042\n",
      "loss = 2.01345842587645\n",
      "loss = 2.0130565844466215\n",
      "loss = 2.0126626849493907\n",
      "loss = 2.012276570328948\n",
      "loss = 2.0118981859109653\n",
      "loss = 2.011527419793936\n",
      "loss = 2.011163982902228\n",
      "loss = 2.0108077912677165\n",
      "loss = 2.010458958680976\n",
      "loss = 2.010117024382361\n",
      "loss = 2.0097818503744813\n",
      "loss = 2.009453302111466\n",
      "loss = 2.00913161930261\n",
      "loss = 2.008816996889261\n",
      "loss = 2.008509738090253\n",
      "loss = 2.008208668149552\n",
      "loss = 2.007913570331906\n",
      "loss = 2.0076243157021434\n",
      "loss = 2.007340782596076\n",
      "loss = 2.0070639501906298\n",
      "loss = 2.0067936699862896\n",
      "loss = 2.0065287905432014\n",
      "loss = 2.0062691814655835\n",
      "loss = 2.0060147245579594\n",
      "loss = 2.0057653096602617\n",
      "loss = 2.0055208321654447\n",
      "loss = 2.005281191533746\n",
      "loss = 2.00504629039745\n",
      "loss = 2.004816034015647\n",
      "loss = 2.004590329936539\n",
      "loss = 2.004369208129397\n",
      "loss = 2.004153836896327\n",
      "loss = 2.0039428588108144\n",
      "loss = 2.003736065322113\n",
      "loss = 2.0035333678618588\n",
      "loss = 2.003334681918938\n",
      "loss = 2.0031407342322742\n",
      "loss = 2.0029506974988256\n",
      "loss = 2.0027644274598075\n",
      "loss = 2.002582245598269\n",
      "loss = 2.002404043700971\n",
      "loss = 2.002229702383865\n",
      "loss = 2.002059228270858\n",
      "loss = 2.001892140303755\n",
      "loss = 2.00172891412343\n",
      "loss = 2.0015691514491034\n",
      "loss = 2.0014125636486906\n",
      "loss = 2.001259082123943\n",
      "loss = 2.001108641774746\n",
      "loss = 2.000961180139228\n",
      "loss = 2.0008166368429117\n",
      "loss = 2.000674953242931\n",
      "loss = 2.0005360721954357\n",
      "loss = 2.0004005630539172\n",
      "loss = 2.0002678803842\n",
      "loss = 2.000137825825717\n",
      "loss = 2.0000103453171967\n",
      "loss = 1.9998853866287953\n",
      "loss = 1.999762899049463\n",
      "loss = 1.9996429612887836\n",
      "loss = 1.999526228614325\n",
      "loss = 1.999412426993869\n",
      "loss = 1.999301236965621\n",
      "loss = 1.9991922615350768\n",
      "loss = 1.9990854506873663\n",
      "loss = 1.9989812771109037\n",
      "loss = 1.998880552472056\n",
      "loss = 1.9987818632407577\n",
      "loss = 1.9986851544406983\n",
      "loss = 1.998591308874584\n",
      "loss = 1.9985013242737943\n",
      "loss = 1.9984134548172714\n",
      "loss = 1.9983274056350222\n",
      "loss = 1.9982431156640845\n",
      "loss = 1.998160991764754\n",
      "loss = 1.9980813670418947\n",
      "loss = 1.9980040139343154\n",
      "loss = 1.9979297835558811\n",
      "loss = 1.9978577835557145\n",
      "loss = 1.9977874470574806\n",
      "loss = 1.997721346269659\n",
      "loss = 1.997657346469087\n",
      "loss = 1.9975957419130648\n",
      "loss = 1.99753552129407\n",
      "loss = 1.997477028559129\n",
      "loss = 1.9974209531274942\n",
      "loss = 1.9973666128485692\n",
      "loss = 1.997316204233017\n",
      "loss = 1.9972672409005738\n",
      "loss = 1.99721943059147\n",
      "loss = 1.9971727050776673\n",
      "loss = 1.9971280820646564\n",
      "loss = 1.9970856434394897\n",
      "loss = 1.9970447166397658\n",
      "loss = 1.9970072367391432\n",
      "loss = 1.9969716651732443\n",
      "loss = 1.9969370188975584\n",
      "loss = 1.9969032302400345\n",
      "loss = 1.9968702439259585\n",
      "loss = 1.9968380143541755\n",
      "loss = 1.9968065034827325\n",
      "loss = 1.9967756791873597\n",
      "loss = 1.9967455139868313\n",
      "loss = 1.9967159840529984\n",
      "loss = 1.9966870684417066\n",
      "loss = 1.996659440125272\n",
      "loss = 1.9966342901770828\n",
      "loss = 1.9966097061892958\n",
      "loss = 1.99658566272203\n",
      "loss = 1.9965621379053975\n",
      "loss = 1.9965391127064231\n",
      "loss = 1.996516570353615\n",
      "loss = 1.99649449588513\n",
      "loss = 1.9964728757938381\n",
      "loss = 1.9964516977483715\n",
      "loss = 1.9964309503737403\n",
      "loss = 1.99641062307867\n",
      "loss = 1.9963907059195691\n",
      "loss = 1.9963711894932301\n",
      "loss = 1.9963520648520656\n",
      "loss = 1.996333323437025\n",
      "loss = 1.99631495702438\n",
      "loss = 1.9962969576834009\n",
      "loss = 1.9962793177425762\n",
      "loss = 1.9962620297625446\n",
      "loss = 1.9962487491924175\n",
      "loss = 1.9962377211744062\n",
      "loss = 1.9962270030820313\n",
      "loss = 1.9962165703280175\n",
      "loss = 1.9962064025006752\n",
      "loss = 1.996196482544298\n",
      "loss = 1.9961867961029942\n",
      "loss = 1.9961773309953175\n",
      "loss = 1.9961680767935697\n",
      "loss = 1.9961590244868757\n",
      "loss = 1.996150166211293\n",
      "loss = 1.996141495033581\n",
      "loss = 1.99613300477791\n",
      "loss = 1.9961246898869336\n",
      "loss = 1.996116545310376\n",
      "loss = 1.9961085664156295\n",
      "loss = 1.996100748915983\n",
      "loss = 1.9960930888129507\n",
      "loss = 1.9960855823499015\n",
      "loss = 1.9960782259747287\n",
      "loss = 1.996071016309758\n",
      "loss = 1.9960639501274582\n",
      "loss = 1.9960570243307931\n",
      "loss = 1.9960502359372962\n",
      "loss = 1.9960435820661235\n",
      "loss = 1.9960370599274992\n",
      "loss = 1.9960306668140775\n",
      "loss = 1.9960244000938419\n",
      "loss = 1.996018257204235\n",
      "loss = 1.9960122356472831\n",
      "loss = 1.9960063329855182\n",
      "loss = 1.996000546838534\n",
      "loss = 1.995994874880069\n",
      "loss = 1.995989314835499\n",
      "loss = 1.9959838644796646\n",
      "loss = 1.995978521634979\n",
      "loss = 1.9959732841697502\n",
      "loss = 1.9959681499966866\n",
      "loss = 1.995963117071551\n",
      "loss = 1.9959581833919304\n",
      "loss = 1.9959533469961135\n",
      "loss = 1.99594860596204\n",
      "loss = 1.9959439584063314\n",
      "loss = 1.9959394024833694\n",
      "loss = 1.9959349363844276\n",
      "loss = 1.9959305583368454\n",
      "loss = 1.9959262666032374\n",
      "loss = 1.9959220594807339\n",
      "loss = 1.9959179353002487\n",
      "loss = 1.9959138924257736\n",
      "loss = 1.9959099292536924\n",
      "loss = 1.9959060442121144\n",
      "loss = 1.9959022357602323\n",
      "loss = 1.9958985023876885\n",
      "loss = 1.9958948426139627\n",
      "loss = 1.9958912549877712\n",
      "loss = 1.9958877380864861\n",
      "loss = 1.995884290515557\n",
      "loss = 1.9958809109079574\n",
      "loss = 1.9958775979236338\n",
      "loss = 1.9958743502489722\n",
      "loss = 1.9958711665962734\n",
      "loss = 1.9958680457032385\n",
      "loss = 1.995864986332468\n",
      "loss = 1.9958619872709689\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-38c3fe64d68f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoft_thresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_prox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def soft_thresh(lam, alpha, beta):\n",
    "    m, _ = np.shape(beta)\n",
    "    s_t = np.zeros((m,1))\n",
    "    for i in range(m):\n",
    "        if beta[i] > lam * alpha:\n",
    "            s_t[i,0] = beta[i] - lam * alpha\n",
    "        elif beta[i] < - lam * alpha:\n",
    "            s_t[i,0] = beta[i] + lam * alpha\n",
    "        else:\n",
    "            s_t[i,0] = 0\n",
    "    return s_t\n",
    "\n",
    "\n",
    "alpha = 0.001\n",
    "beta = np.empty_like(beta_0)\n",
    "beta[:] = beta_0\n",
    "L = loss(X, y, beta, lam)\n",
    "L3_store = []\n",
    "c3_store = []\n",
    "counter = 0\n",
    "while L > thresh:\n",
    "    counter += 1\n",
    "    beta = beta + alpha * (1/N) * matmul(np.transpose(X), (y - matmul(X, beta)))\n",
    "    beta = soft_thresh(lam, alpha, beta)       \n",
    "    L = loss(X, y, beta, lam)\n",
    "    counter += 1\n",
    "    if counter % 10 == 0:\n",
    "        L3_store.append(L)\n",
    "        c3_store.append(counter)\n",
    "        print (\"loss = {}\".format(L))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Implement proximal gradient descent with backtracking line search. You can find more about backtracking line search in https://www.robots.ox.ac. uk/~vgg/rg/slides/fgrad.pdf. Plot the objective function with growing number of iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "g() missing 2 required positional arguments: 'y' and 'beta'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-eefaf5c913a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mbeta_plus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta_old\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_plus\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_old\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_g\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbeta_plus\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_plus\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbeta_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0malpha\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: g() missing 2 required positional arguments: 'y' and 'beta'"
     ]
    }
   ],
   "source": [
    "alpha_0 = 0.001\n",
    "multiplier = 0.9\n",
    "beta = np.empty_like(beta_0)\n",
    "beta[:] = beta_0\n",
    "L = loss(X, y, beta, lam)\n",
    "L4_store = []\n",
    "c4_store = []\n",
    "counter = 0\n",
    "\n",
    "def g(beta):\n",
    "#     set_trace()\n",
    "    return   (1/N) * ( 0.5 * norm((matmul(X,beta)-y), ord=2)**2 )\n",
    "\n",
    "def grad_g(beta, lam):\n",
    "    return   (1/N) * ( matmul(np.transpose(X),(matmul(X,beta)-y)) )\n",
    "\n",
    "def grad(X, y, beta, lam):\n",
    "    return   (1/N) * ( matmul(np.transpose(X),(matmul(X,beta)-y)) + lam * np.sign(beta) )\n",
    "\n",
    "# backtracking line search\n",
    "while L > thresh:\n",
    "    counter += 1\n",
    "    beta_old = np.empty_like(beta)\n",
    "    beta_old[:] = beta\n",
    "    while True:\n",
    "        beta_plus = beta_old + alpha * (1/N) * matmul(np.transpose(X), (y - matmul(X, beta_old)))\n",
    "        value = g(beta_plus) <= g(beta_old) + matmul(np.transpose(grad_g(beta_old)), (beta_plus - beta_old)) + (1/(2*alpha)) * norm(beta_plus-beta_old)**2\n",
    "        if value:\n",
    "            alpha *= multiplier\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    beta = beta + alpha * (1/N) * matmul(np.transpose(X), (y - matmul(X, beta)))\n",
    "    beta = soft_thresh(lam, alpha, beta)       \n",
    "    L = loss(X, y, beta, lam)\n",
    "    counter += 1\n",
    "    if counter % 10 == 0:\n",
    "        L3_store.append(L)\n",
    "        c3_store.append(counter)\n",
    "        print (\"loss = {}\".format(L))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
